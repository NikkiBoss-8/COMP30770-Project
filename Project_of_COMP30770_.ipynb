{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmWcJTQGniUt6jL8X5uT29",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikkiBoss-8/COMP30770-Project/blob/main/Project_of_COMP30770_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install Required Libraries\n",
        "Start by installing any necessary libraries (e.g., PySpark). This ensures that your environment is set up correctly."
      ],
      "metadata": {
        "id": "8nG8ezuRqUJN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR-reUMSqPuZ",
        "outputId": "3eda6aeb-36df-4697-960c-9bf6c92e5933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import Libraries\n",
        "Import all the required libraries at the beginning of your notebook."
      ],
      "metadata": {
        "id": "5ULp72GUqe87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "bdagq7fAqVTc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Traditional Solution\n",
        "Include the code for your traditional solution (without MapReduce)."
      ],
      "metadata": {
        "id": "R8iZugeyqrJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Decomposition of Tasks: Data Cleaning: Check for missing or invalid values.\n",
        "!awk -F',' 'NF==14' ItalianWine.csv > cleaned_data.cs"
      ],
      "metadata": {
        "id": "OX6KfWqwu98i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Scaling:   Scale numerical features to have zero mean and unit variance.\n",
        "%%writefile mapper.py\n",
        "\n",
        " # !/usr/bin/python3\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"ItalianWine.csv\")\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data.iloc[:, :-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgzcWFiMvjfp",
        "outputId": "fecbbbd4-73fa-4683-b4c0-19ae5e50ec1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimensionality Reduction:  Apply Principal Component Analysis (PCA) to reduce the number of features.\n",
        "%%writefile mapper.py\n",
        "\n",
        "# !/usr/bin/python3\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=5)\n",
        "reduced_data = pca.fit_transform(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smh2u86SvzGt",
        "outputId": "099e0b9c-4d81-4848-8adc-ca25d0cc2589"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Training:  Train a Random Forest classifier to predict wine classes.\n",
        "%%writefile mapper.py\n",
        "\n",
        "# !/usr/bin/python3\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reduced_data, data['label'], test_size=0.2)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkJqSivwwAZ9",
        "outputId": "9862e6bc-9e36-44cf-d2a3-b0d971362abd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading our hourly data into a PySpark DataFrame:\n",
        "df = spark.read.options(delimiter=\",\", header=True, inferSchema=True).csv(\"/content/ItalianWine.csv\")"
      ],
      "metadata": {
        "id": "4pnKfBfn2NDc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see some more information about our data:\n",
        "# The inferSchema automatically guesses the datatypes\n",
        "# Otherwise it would assume all datatypes are strings\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPHxI-bX6ERG",
        "outputId": "e01a1f02-d820-4b6f-8779-c8c58d641d3e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Alcohol: double (nullable = true)\n",
            " |-- Malic acid: double (nullable = true)\n",
            " |-- Ash: double (nullable = true)\n",
            " |-- Alcalinity of ash: double (nullable = true)\n",
            " |-- Magnesium: double (nullable = true)\n",
            " |-- Total phenols: double (nullable = true)\n",
            " |-- Flavanoids: double (nullable = true)\n",
            " |-- Nonflavanoid phenols: double (nullable = true)\n",
            " |-- Proanthocyanins: double (nullable = true)\n",
            " |-- Color intensitys: double (nullable = true)\n",
            " |-- Hue: double (nullable = true)\n",
            " |-- OD280/OD315 of diluted wines: double (nullable = true)\n",
            " |-- Proline: double (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation:    Evaluate the model using accuracy and confusion matrix.\n",
        "%%writefile mapper.py\n",
        "\n",
        "# !/usr/bin/python3\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAcyRv-gwUn8",
        "outputId": "889ac7e4-c024-418a-8a41-258c8c38d198"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. MapReduce Solution\n",
        "Include the code for your MapReduce solution."
      ],
      "metadata": {
        "id": "_aBpM8yAq4PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As Spark is written in the Scala programming language, it requires the JVM to run. So our first step is to install Java:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "C7t659U-yB28"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next we need to download and upack Apache Spark and Hadoop:\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
        "!rm spark-3.3.1-bin-hadoop3.tgz   # Tidying up\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aELsWvPhyh9V",
        "outputId": "d21e49ac-0700-4d6f-d1b4-6887bb5df7d6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-16 17:06:57--  https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299350810 (285M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.3.1-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.3.1-bin-had 100%[===================>] 285.48M  18.5MB/s    in 16s     \n",
            "\n",
            "2025-03-16 17:07:14 (17.3 MB/s) - ‘spark-3.3.1-bin-hadoop3.tgz’ saved [299350810/299350810]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up our environmental variables:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "TA6U1jONywJS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next we'll need to install the findpark library to locate Spark on the system:\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Z35xdohky2_p"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can now import SparkSession from pyspark.sql to create our entry point to Spark.\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) #  This will format our output tables a bit nicer when not using the show() method\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "AvGFks3RzE3i",
        "outputId": "aaa8f35a-3671-41b7-f601-704c4389a68f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bb81567b450>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5cdee7630291:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You can check how many CPU cores are available to you.\n",
        "import multiprocessing\n",
        "print(multiprocessing.cpu_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLx-Y37QzY8Q",
        "outputId": "7a381181-b6a5-4acd-93c4-431ac76156a8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload the File to Colab: if nessecarey\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Nk5kLZlz0vvz",
        "outputId": "4dc97826-4104-47a5-96f6-5489f4c5603d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e2a68d8f-306d-4241-8295-eae819588a96\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e2a68d8f-306d-4241-8295-eae819588a96\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ItalianWine.csv to ItalianWine (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify the File Path:\n",
        "!ls /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATAacGmU09N4",
        "outputId": "baae557e-aa85-448f-b581-cb1fbba27866"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " cleaned_data.cs        ItalianWine.csv   output_scaled_data_1742141345   spark-3.3.1-bin-hadoop3\n",
            "'ItalianWine (1).csv'   mapper.py\t  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the Correct File Path:\n",
        "!data = sc.textFile(\"/content/ItalianWine.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLx0cHjN1HVW",
        "outputId": "bf8c9e92-9709-4549-8943-dc9643638ad6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `data = sc.textFile(\"/content/ItalianWine.csv\")'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading our hourly data into a PySpark DataFrame:\n",
        "df = spark.read.options(delimiter=\",\", header=True, inferSchema=True).csv(\"/content/ItalianWine.csv\")\n",
        "# First we'll show just the first 10 records:\n",
        "df.select(\"*\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOHzIr4i1QXL",
        "outputId": "01e3cb48-920b-443e-f6b4-832d0b7d6a1f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "|Alcohol|Malic acid| Ash|Alcalinity of ash|Magnesium|Total phenols|Flavanoids|Nonflavanoid phenols|Proanthocyanins|Color intensitys| Hue|OD280/OD315 of diluted wines|Proline|label|\n",
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "|  14.23|      1.71|2.43|             15.6|    127.0|          2.8|      3.06|                0.28|           2.29|            5.64|1.04|                        3.92| 1065.0|    1|\n",
            "|   13.2|      1.78|2.14|             11.2|    100.0|         2.65|      2.76|                0.26|           1.28|            4.38|1.05|                         3.4| 1050.0|    1|\n",
            "|  13.16|      2.36|2.67|             18.6|    101.0|          2.8|      3.24|                 0.3|           2.81|            5.68|1.03|                        3.17| 1185.0|    1|\n",
            "|  14.37|      1.95| 2.5|             16.8|    113.0|         3.85|      3.49|                0.24|           2.18|             7.8|0.86|                        3.45| 1480.0|    1|\n",
            "|  13.24|      2.59|2.87|             21.0|    118.0|          2.8|      2.69|                0.39|           1.82|            4.32|1.04|                        2.93|  735.0|    1|\n",
            "|   14.2|      1.76|2.45|             15.2|    112.0|         3.27|      3.39|                0.34|           1.97|            6.75|1.05|                        2.85| 1450.0|    1|\n",
            "|  14.39|      1.87|2.45|             14.6|     96.0|          2.5|      2.52|                 0.3|           1.98|            5.25|1.02|                        3.58| 1290.0|    1|\n",
            "|  14.06|      2.15|2.61|             17.6|    121.0|          2.6|      2.51|                0.31|           1.25|            5.05|1.06|                        3.58| 1295.0|    1|\n",
            "|  14.83|      1.64|2.17|             14.0|     97.0|          2.8|      2.98|                0.29|           1.98|             5.2|1.08|                        2.85| 1045.0|    1|\n",
            "|  13.86|      1.35|2.27|             16.0|     98.0|         2.98|      3.15|                0.22|           1.85|            7.22|1.01|                        3.55| 1045.0|    1|\n",
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if the file exists and is accessible\n",
        "import os\n",
        "\n",
        "file_path = \"/content/ItalianWine.csv\"\n",
        "if os.path.exists(file_path):\n",
        "    print(\" File exists:\", file_path)\n",
        "else:\n",
        "    print(\" File NOT found. Check the file path!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn3GUW0TAG25",
        "outputId": "91a87c2c-69d3-4fca-a5b2-d1fc107e0166"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " File exists: /content/ItalianWine.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify Java Installation Check if Java is installed correctly:\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWkbi_baDxk9",
        "outputId": "fe0bd1fd-1198-481b-c645-aadef72ae30a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.26\" 2025-01-21\n",
            "OpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "ZwngOkWuD4dE"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n",
        "#Fix data = sc.textFile(\"/content/ItalianWine.csv\")\n",
        "#Make sure the dataset actually exists and is properly formatted. Try checking the file:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWKr69AUD82c",
        "outputId": "e41e6aa4-5180-4074-de3a-4d557b1dbf65"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " cleaned_data.cs\t   ItalianWine.csv\t\t   sample_data\n",
            "'ItalianWine (1).csv'\t   mapper.py\t\t\t   spark-3.3.1-bin-hadoop3\n",
            " ItalianWine_cleaned.csv   output_scaled_data_1742141345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 /content/ItalianWine.csv\n",
        "#If ItalianWine.csv is missing or empty, re-upload it and verify the contents using:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm6g6WKYEDQP",
        "outputId": "527648c8-2a75-4eda-e769-406748c21fad"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alcohol,Malic acid,Ash,Alcalinity of ash,Magnesium,Total phenols,Flavanoids,Nonflavanoid phenols,Proanthocyanins,Color intensitys,Hue,OD280/OD315 of diluted wines,Proline,label\r\n",
            "14.23,1.71,2.43,15.6,127.0,2.8,3.06,0.28,2.29,5.64,1.04,3.92,1065.0,1\r\n",
            "13.2,1.78,2.14,11.2,100.0,2.65,2.76,0.26,1.28,4.38,1.05,3.4,1050.0,1\r\n",
            "13.16,2.36,2.67,18.6,101.0,2.8,3.24,0.3,2.81,5.68,1.03,3.17,1185.0,1\r\n",
            "14.37,1.95,2.5,16.8,113.0,3.85,3.49,0.24,2.18,7.8,0.86,3.45,1480.0,1\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If the file is malformed, load it using Pandas and write it back:\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/ItalianWine.csv\")\n",
        "df.to_csv(\"/content/ItalianWine_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Phf4civ8EIW2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = sc.textFile(\"/content/ItalianWine_cleaned.csv\")\n",
        "#Then, update your Spark code:"
      ],
      "metadata": {
        "id": "56uAQGnoENgG"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure PySpark is Running Properly Since PySpark runs on Java, confirm the environment is working by running:\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
        "print(\"Spark is running!\")\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg1SY_qyERgh",
        "outputId": "e614d24d-0fee-4995-f264-d8b8d1af9170"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark is running!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Ensure previous Spark sessions are stopped before creating a new one\n",
        "spark = SparkSession.builder.appName(\"ItalianWineAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the CSV file correctly using Spark DataFrame\n",
        "df_spark = spark.read.csv(\"/content/ItalianWine.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the first few rows\n",
        "df_spark.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lPYxFJjE5OA",
        "outputId": "612fa37a-d183-4623-a965-5fecb46ce4ff"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "|Alcohol|Malic acid| Ash|Alcalinity of ash|Magnesium|Total phenols|Flavanoids|Nonflavanoid phenols|Proanthocyanins|Color intensitys| Hue|OD280/OD315 of diluted wines|Proline|label|\n",
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "|  14.23|      1.71|2.43|             15.6|    127.0|          2.8|      3.06|                0.28|           2.29|            5.64|1.04|                        3.92| 1065.0|    1|\n",
            "|   13.2|      1.78|2.14|             11.2|    100.0|         2.65|      2.76|                0.26|           1.28|            4.38|1.05|                         3.4| 1050.0|    1|\n",
            "|  13.16|      2.36|2.67|             18.6|    101.0|          2.8|      3.24|                 0.3|           2.81|            5.68|1.03|                        3.17| 1185.0|    1|\n",
            "|  14.37|      1.95| 2.5|             16.8|    113.0|         3.85|      3.49|                0.24|           2.18|             7.8|0.86|                        3.45| 1480.0|    1|\n",
            "|  13.24|      2.59|2.87|             21.0|    118.0|          2.8|      2.69|                0.39|           1.82|            4.32|1.04|                        2.93|  735.0|    1|\n",
            "|   14.2|      1.76|2.45|             15.2|    112.0|         3.27|      3.39|                0.34|           1.97|            6.75|1.05|                        2.85| 1450.0|    1|\n",
            "|  14.39|      1.87|2.45|             14.6|     96.0|          2.5|      2.52|                 0.3|           1.98|            5.25|1.02|                        3.58| 1290.0|    1|\n",
            "|  14.06|      2.15|2.61|             17.6|    121.0|          2.6|      2.51|                0.31|           1.25|            5.05|1.06|                        3.58| 1295.0|    1|\n",
            "|  14.83|      1.64|2.17|             14.0|     97.0|          2.8|      2.98|                0.29|           1.98|             5.2|1.08|                        2.85| 1045.0|    1|\n",
            "|  13.86|      1.35|2.27|             16.0|     98.0|         2.98|      3.15|                0.22|           1.85|            7.22|1.01|                        3.55| 1045.0|    1|\n",
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark if not already installed\n",
        "!pip install pyspark\n",
        "\n",
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# Create a Spark session (prevents multiple SparkContexts issue)\n",
        "spark = SparkSession.builder.appName(\"ItalianWineAnalysis\").getOrCreate()\n",
        "sc = spark.sparkContext  # Get SparkContext from SparkSession\n",
        "\n",
        "# File path for the dataset\n",
        "file_path = \"/content/ItalianWine.csv\"\n",
        "\n",
        "# Ensure the dataset exists\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "# Load the dataset\n",
        "data = sc.textFile(file_path)\n",
        "\n",
        "# Extract and remove the header safely\n",
        "#header = data.first()\n",
        "data = data.filter(lambda row: row != header)\n",
        "\n",
        "# Function to scale features\n",
        "def scale_features(row):\n",
        "    try:\n",
        "        # Convert row into features (excluding the label)\n",
        "        values = row.split(',')\n",
        "        features = np.array(values[:-1], dtype=float)\n",
        "        label = values[-1]  # Extract label (assumed last column)\n",
        "\n",
        "        # Compute mean and standard deviation safely\n",
        "        mean = np.mean(features)\n",
        "        std = np.std(features) if np.std(features) != 0 else 1  # Prevent division by zero\n",
        "\n",
        "        # Apply feature scaling (zero mean, unit variance)\n",
        "        scaled_features = (features - mean) / std\n",
        "\n",
        "        # Return formatted row with scaled features\n",
        "        return ','.join(map(str, scaled_features)) + ',' + label\n",
        "    except Exception as e:\n",
        "        return f\"Error processing row: {row} -> {str(e)}\"\n",
        "\n",
        "# Apply feature scaling transformation\n",
        "scaled_data = data.map(scale_features)\n",
        "\n",
        "# Filter out any errors that may have occurred during processing\n",
        "scaled_data = scaled_data.filter(lambda row: not row.startswith(\"Error\"))\n",
        "\n",
        "# Generate a unique output directory name using a timestamp\n",
        "output_dir = f\"/content/output_scaled_data_{int(time.time())}\"\n",
        "\n",
        "# Save the transformed dataset\n",
        "scaled_data.saveAsTextFile(output_dir)\n",
        "print(f\"Processed data saved to: {output_dir}\")\n",
        "\n",
        "# Stop SparkSession (no need to stop SparkContext explicitly)\n",
        "spark.stop()\n",
        "end_time = time.time()\n",
        "print(f\"Execution Time: {end_time - start_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pltCh45XIUhk",
        "outputId": "fadee883-50ea-4e91-d88e-3f55fa057b4a"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%%writefile` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. View Results\n",
        "Add a section to view the results of your processing. This helps verify that the code is working as expected."
      ],
      "metadata": {
        "id": "bbep2Hpiq9f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Ensure previous Spark sessions are stopped before creating a new one\n",
        "spark = SparkSession.builder.appName(\"ItalianWineAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the CSV file correctly using Spark DataFrame\n",
        "df_spark = spark.read.csv(\"/content/ItalianWine.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the first few rows\n",
        "df_spark.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taQ4ME0Bq7_U",
        "outputId": "42036721-ad91-4423-81d9-e153a908fab5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "|Alcohol|Malic acid| Ash|Alcalinity of ash|Magnesium|Total phenols|Flavanoids|Nonflavanoid phenols|Proanthocyanins|Color intensitys| Hue|OD280/OD315 of diluted wines|Proline|label|\n",
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "|  14.23|      1.71|2.43|             15.6|    127.0|          2.8|      3.06|                0.28|           2.29|            5.64|1.04|                        3.92| 1065.0|    1|\n",
            "|   13.2|      1.78|2.14|             11.2|    100.0|         2.65|      2.76|                0.26|           1.28|            4.38|1.05|                         3.4| 1050.0|    1|\n",
            "|  13.16|      2.36|2.67|             18.6|    101.0|          2.8|      3.24|                 0.3|           2.81|            5.68|1.03|                        3.17| 1185.0|    1|\n",
            "|  14.37|      1.95| 2.5|             16.8|    113.0|         3.85|      3.49|                0.24|           2.18|             7.8|0.86|                        3.45| 1480.0|    1|\n",
            "|  13.24|      2.59|2.87|             21.0|    118.0|          2.8|      2.69|                0.39|           1.82|            4.32|1.04|                        2.93|  735.0|    1|\n",
            "|   14.2|      1.76|2.45|             15.2|    112.0|         3.27|      3.39|                0.34|           1.97|            6.75|1.05|                        2.85| 1450.0|    1|\n",
            "|  14.39|      1.87|2.45|             14.6|     96.0|          2.5|      2.52|                 0.3|           1.98|            5.25|1.02|                        3.58| 1290.0|    1|\n",
            "|  14.06|      2.15|2.61|             17.6|    121.0|          2.6|      2.51|                0.31|           1.25|            5.05|1.06|                        3.58| 1295.0|    1|\n",
            "|  14.83|      1.64|2.17|             14.0|     97.0|          2.8|      2.98|                0.29|           1.98|             5.2|1.08|                        2.85| 1045.0|    1|\n",
            "|  13.86|      1.35|2.27|             16.0|     98.0|         2.98|      3.15|                0.22|           1.85|            7.22|1.01|                        3.55| 1045.0|    1|\n",
            "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+----------------+----+----------------------------+-------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Performance Comparison\n",
        "Add a section to compare the performance of the traditional solution and the MapReduce solution. You can use Python's time module to measure execution time."
      ],
      "metadata": {
        "id": "0LgUDgN_rGbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Traditional Solution\n",
        "start_time = time.time()\n",
        "# Insert traditional solution code here\n",
        "end_time = time.time()\n",
        "print(f\"Traditional Solution Execution Time: {end_time - start_time} seconds\")\n",
        "\n",
        "# MapReduce Solution\n",
        "start_time = time.time()\n",
        "# Insert MapReduce solution code here\n",
        "end_time = time.time()\n",
        "print(f\"MapReduce Solution Execution Time: {end_time - start_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0lBf7zvrC0-",
        "outputId": "12a1a6a5-884b-41a7-a09a-741fbde9fc68"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traditional Solution Execution Time: 2.8371810913085938e-05 seconds\n",
            "MapReduce Solution Execution Time: 2.574920654296875e-05 seconds\n"
          ]
        }
      ]
    }
  ]
}